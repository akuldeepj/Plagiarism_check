{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./env/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.11/site-packages (1.26.2)\n",
      "Requirement already satisfied: matplotlib in ./env/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: seaborn in ./env/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: transformers in ./env/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: torch in ./env/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./env/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./env/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./env/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./env/lib/python3.11/site-packages (from matplotlib) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./env/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in ./env/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./env/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./env/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./env/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./env/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions in ./env/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Use this If the dataset has all the data in the format of a single csv file\n",
    "#Num    abstract\tcord_uid\n",
    "%pip install pandas numpy matplotlib seaborn transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_path, sample_size):\n",
    "\n",
    "\n",
    "  data = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "\n",
    "  data = data.dropna(subset = ['abstract']).reset_index(drop = True)\n",
    "\n",
    "\n",
    "  data = data.sample(sample_size)[['abstract', 'cord_uid']]\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdataset\u001b[m\u001b[m/             script.ipynb         vector_database.csv\n",
      "\u001b[34menv\u001b[m\u001b[m/                 test.py\n",
      "\u001b[34mmodel\u001b[m\u001b[m/               \u001b[34mtokenizer\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/akuldeepj/Desktop/code/LLM_plagerism/dataset/metadata.csv\"\n",
    "source_data = preprocess_data(data_path, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68027</th>\n",
       "      <td>The lockdown of the country, imposed by the go...</td>\n",
       "      <td>xug40l4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82707</th>\n",
       "      <td>During the current COVID-19 pandemic, the rapi...</td>\n",
       "      <td>dck2gn3f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400080</th>\n",
       "      <td>Comparing first and second wave MIS-C cohorts ...</td>\n",
       "      <td>as2cayui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48153</th>\n",
       "      <td>This paper voices the opinions of internationa...</td>\n",
       "      <td>x82982pd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270030</th>\n",
       "      <td>We used a quantitative microbial risk assessme...</td>\n",
       "      <td>7sfmk8eq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 abstract  cord_uid\n",
       "68027   The lockdown of the country, imposed by the go...  xug40l4o\n",
       "82707   During the current COVID-19 pandemic, the rapi...  dck2gn3f\n",
       "400080  Comparing first and second wave MIS-C cohorts ...  as2cayui\n",
       "48153   This paper voices the opinions of internationa...  x82982pd\n",
       "270030  We used a quantitative microbial risk assessme...  7sfmk8eq"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./env/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: keras in ./env/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: torch in ./env/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in ./env/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: tensorflow in ./env/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos in ./env/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-metal in ./env/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./env/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./env/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./env/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions in ./env/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (4.23.4)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.11/site-packages (from tensorflow-macos) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (2.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in ./env/lib/python3.11/site-packages (from tensorflow-macos) (2.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in ./env/lib/python3.11/site-packages (from tensorflow-metal) (0.41.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./env/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./env/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos) (3.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./env/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./env/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./env/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./env/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./env/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./env/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./env/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./env/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers keras torch scikit-learn tensorflow tensorflow-macos tensorflow-metal\n",
    "import torch\n",
    "from keras.utils import pad_sequences\n",
    "from transformers import BertTokenizer,  AutoModelForSequenceClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdataset\u001b[m\u001b[m/             script.ipynb         vector_database.csv\n",
      "\u001b[34menv\u001b[m\u001b[m/                 test.py\n",
      "\u001b[34mmodel\u001b[m\u001b[m/               \u001b[34mtokenizer\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_from_text(tokenizer, model, text, MAX_LEN = 510):\n",
    "    \n",
    "    input_ids = tokenizer.encode(\n",
    "                        text, \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = MAX_LEN,                           \n",
    "                   )    \n",
    "\n",
    "    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\", \n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    \n",
    "    input_ids = results[0]\n",
    "\n",
    "    \n",
    "    attention_mask = [int(i>0) for i in input_ids]\n",
    "    \n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    \n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        logits, encoded_layers = model(\n",
    "                                    input_ids = input_ids, \n",
    "                                    token_type_ids = None, \n",
    "                                    attention_mask = attention_mask,\n",
    "                                    return_dict=False)\n",
    "\n",
    "    layer_i = 12 \n",
    "    batch_i = 0 \n",
    "    token_i = 0 \n",
    "        \n",
    "    # Embedding.\n",
    "    vector = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "\n",
    "    vector = vector.detach().cpu().numpy()\n",
    "\n",
    "    return(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_vector_database(data):\n",
    "    \n",
    "    \n",
    "    vectors = []\n",
    "    \n",
    "    \n",
    "    source_data = data.abstract.values\n",
    "    \n",
    "    \n",
    "    for text in tqdm(source_data):\n",
    "        \n",
    "    \n",
    "        vector = create_vector_from_text(tokenizer, model, text)\n",
    "        \n",
    "        \n",
    "        vectors.append(vector)\n",
    "    \n",
    "    data[\"vectors\"] = vectors\n",
    "    data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: np.array(emb))\n",
    "    data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: emb.reshape(1, -1))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 100/100 [00:15<00:00,  6.41it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_database = create_vector_database(source_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>586612</th>\n",
       "      <td>Porcine hemagglutinating encephalomyelitis vir...</td>\n",
       "      <td>07b3pbxc</td>\n",
       "      <td>[[-0.3795591, -0.348499, -0.78716296, -0.15551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32236</th>\n",
       "      <td>Extrapolating from modern international unders...</td>\n",
       "      <td>fucyt0zc</td>\n",
       "      <td>[[-0.28831905, -0.41381738, -0.8977126, -0.412...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200868</th>\n",
       "      <td>COVID-19 is an important threat worldwide. Thi...</td>\n",
       "      <td>msu81brd</td>\n",
       "      <td>[[-0.6227991, -0.74918747, -0.06620249, -0.415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791353</th>\n",
       "      <td>Trauma survivors who suffer from posttraumatic...</td>\n",
       "      <td>nslygsdf</td>\n",
       "      <td>[[-0.7621305, -0.3840289, -0.671917, -0.228419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503719</th>\n",
       "      <td>Understanding the cell entry mechanism of pand...</td>\n",
       "      <td>muzqgk95</td>\n",
       "      <td>[[-0.28338796, 0.011545289, -0.36038852, -0.50...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 abstract  cord_uid  \\\n",
       "586612  Porcine hemagglutinating encephalomyelitis vir...  07b3pbxc   \n",
       "32236   Extrapolating from modern international unders...  fucyt0zc   \n",
       "200868  COVID-19 is an important threat worldwide. Thi...  msu81brd   \n",
       "791353  Trauma survivors who suffer from posttraumatic...  nslygsdf   \n",
       "503719  Understanding the cell entry mechanism of pand...  muzqgk95   \n",
       "\n",
       "                                                  vectors  \n",
       "586612  [[-0.3795591, -0.348499, -0.78716296, -0.15551...  \n",
       "32236   [[-0.28831905, -0.41381738, -0.8977126, -0.412...  \n",
       "200868  [[-0.6227991, -0.74918747, -0.06620249, -0.415...  \n",
       "791353  [[-0.7621305, -0.3840289, -0.671917, -0.228419...  \n",
       "503719  [[-0.28338796, 0.011545289, -0.36038852, -0.50...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_database.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install sentencepiece\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(text):\n",
    "   \n",
    "    text_vect = create_vector_from_text(tokenizer, model, text)\n",
    "    text_vect = np.array(text_vect)\n",
    "    text_vect = text_vect.reshape(1, -1)\n",
    "\n",
    "    return text_vect\n",
    "\n",
    "    \n",
    "def is_plagiarism(similarity_score, plagiarism_threshold):\n",
    "\n",
    "  is_plagiarism = False\n",
    "\n",
    "  if(similarity_score >= plagiarism_threshold):\n",
    "    is_plagiarism = True\n",
    "\n",
    "  return is_plagiarism\n",
    "\n",
    "\n",
    "\n",
    "def run_plagiarism_analysis(query_text, data, plagiarism_threshold=0.8):\n",
    "\n",
    "    top_N=3\n",
    "\n",
    "    query_vect = process_document(query_text)\n",
    "\n",
    "    data[\"similarity\"] = data[\"vectors\"].apply(lambda x: cosine_similarity(query_vect, x))\n",
    "    data[\"similarity\"] = data[\"similarity\"].apply(lambda x: x[0][0])\n",
    "\n",
    "    similar_articles = data.sort_values(by='similarity', ascending=False)[0:top_N+1]\n",
    "    formated_result = similar_articles[[\"abstract\", \"cord_uid\", \"similarity\"]].reset_index(drop = True)\n",
    "\n",
    "    similarity_score = formated_result.iloc[0][\"similarity\"] \n",
    "    most_similar_article = formated_result.iloc[0][\"abstract\"] \n",
    "    is_plagiarism_bool = is_plagiarism(similarity_score, plagiarism_threshold)\n",
    "\n",
    "    plagiarism_decision = {'similarity_score': similarity_score, \n",
    "                             'is_plagiarism': is_plagiarism_bool,\n",
    "                             'most_similar_article': most_similar_article, \n",
    "                             'article_submitted': query_text\n",
    "                            }\n",
    "\n",
    "    return plagiarism_decision\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_incoming_text = \"My previous experience with flu\"\n",
    "\n",
    "analysis_result = run_plagiarism_analysis(new_incoming_text, vector_database, plagiarism_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similarity_score': 0.7408559, 'is_plagiarism': False, 'most_similar_article': 'For a searchable version of these abstracts, please visit www.acrabstracts.org.', 'article_submitted': 'My previous experience with flu'}\n"
     ]
    }
   ],
   "source": [
    "print(analysis_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plagiarism Analysis Results:\n",
      "Similarity Score: 0.8608219623565674\n",
      "Is Plagiarism: True\n",
      "Most Similar Article:\n",
      "[ ]there is great potential for attitudes and decision-making processes regarding working while sick to change, hopefully for the better [ ]what once was a largely private and personal decision based on a somewhat limited set of antecedents (see Miraglia & Johns, 2016) is now multifaceted, emotionally charged, and perhaps (hopefully?) more morally derived and other-focused than before Research suggests that emphasizing harm to others, as compared with emphasizing harm to oneself, increases engagement in personal safety behaviors (e g , Grant & Hoffman, 2011) [ ]we suggest additional investigation as to whether considering other stakeholders in decision making can be leveraged for changing presenteeism beliefs and behaviors An opportunity for lasting positive change At present, the many ways in which the COVID-19 pandemic has disrupted the world through premature loss of life, declining health, income and job insecurity, social tension, and more are clear my name is akuldeep\n",
      "Article Submitted:\n",
      "Hello I name is ajay and my early life experience has a long lasting experience on social behaviour\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(\"tokenizer\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"model\",\n",
    "                                                                  output_attentions=False,\n",
    "                                                                  output_hidden_states=True)\n",
    "# query_text = \"hello I am Akuldeep the plumber and I repair pipes for a living and I am very good at it\"\n",
    "query_text = \"Hello I name is ajay and my early life experience has a long lasting experience on social behaviour\"\n",
    "def process_document_with_loaded_model(text):\n",
    "    text_vect = create_vector_from_text(loaded_tokenizer, loaded_model, text)\n",
    "    text_vect = np.array(text_vect)\n",
    "    text_vect = text_vect.reshape(1, -1)\n",
    "    return text_vect\n",
    "def run_plagiarism_analysis_with_loaded_model(query_text, data, plagiarism_threshold=0.8):\n",
    "    top_N = 3\n",
    "    query_vect = process_document_with_loaded_model(query_text)\n",
    "\n",
    "    data[\"similarity\"] = data[\"vectors\"].apply(lambda x: cosine_similarity(query_vect, x))\n",
    "    data[\"similarity\"] = data[\"similarity\"].apply(lambda x: x[0][0])\n",
    "\n",
    "    similar_articles = data.sort_values(by='similarity', ascending=False)[0:top_N+1]\n",
    "    formated_result = similar_articles[[\"abstract\", \"cord_uid\", \"similarity\"]].reset_index(drop=True)\n",
    "\n",
    "    similarity_score = formated_result.iloc[0][\"similarity\"]\n",
    "    most_similar_article = formated_result.iloc[0][\"abstract\"]\n",
    "    is_plagiarism_bool = is_plagiarism(similarity_score, plagiarism_threshold)\n",
    "\n",
    "    plagiarism_decision = {'similarity_score': similarity_score,\n",
    "                           'is_plagiarism': is_plagiarism_bool,\n",
    "                           'most_similar_article': most_similar_article,\n",
    "                           'article_submitted': query_text\n",
    "                          }\n",
    "\n",
    "    print(\"Plagiarism Analysis Results:\")\n",
    "    print(f\"Similarity Score: {similarity_score}\")\n",
    "    print(f\"Is Plagiarism: {is_plagiarism_bool}\")\n",
    "    print(f\"Most Similar Article:\\n{most_similar_article}\")\n",
    "    print(f\"Article Submitted:\\n{query_text}\")\n",
    "\n",
    "    return plagiarism_decision\n",
    "plagiarism_result = run_plagiarism_analysis_with_loaded_model(query_text, vector_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_database.to_csv(\"vector_database.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
